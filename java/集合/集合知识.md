**ArrayList：**

- 底层实现是数组
- ArrayList的默认初**始化容量是10**，每次扩容时候增加原先容量的一半，也就是变为原来的1.5倍
- 在**增删时候，需要数组的拷贝复制(navite 方法由C/C++实现)**

**LinkedList：**

- 底层实现是**双向链表**[双向链表方便实现往前遍历]

**Vector：**

- 底层是数组，现在已少用，被ArrayList替代，原因有两个：

- - Vector所有方法都是同步，**有性能损失**。
  - Vector初始length是10 超过length时 以100%比率增长，**相比于ArrayList更多消耗内存**。
  - 参考资料：https://www.zhihu.com/question/31948523/answer/113357347

**总的来说：查询多用ArrayList，增删多用LinkedList。**

**ArrayList增删慢不是绝对**的(**在数量大的情况下，已测试**)：

- 如果增加元素一直是使用add()(增加到末尾)的话，那是ArrayList要快
- 一直**删除末尾的元素也是ArrayList要快**【不用复制移动位置】
- 至于如果**删除的是中间的位置的话，还是ArrayList要快**！

**HashMap:**

在JDK8中HashMap的底层是：**数组+链表(散列表)+红黑树**

在散列表中有装载因子这么一个属性，当装载因子*初始容量小于散列表元素时，该散列表会再散列，扩容2倍！

装载因子的**默认值是0.75**，无论是初始大了还是初始小了对我们HashMap的性能都不好

- 装载因子初始值大了，可以减少散列表再散列(扩容的次数)，但同时会导致散列冲突的可能性变大(**散列冲突也是耗性能的一个操作，要得操作链表(红黑树)**！
- 装载因子初始值小了，可以减小散列冲突的可能性，但同时扩容的次数可能就会变多！

初始容量的**默认值是16**，它也一样，无论初始大了还是小了，对我们的HashMap都是有影响的：

- 初始容量过大，那么遍历时我们的速度就会受影响~
- 初始容量过小，散列表再散列(扩容的次数)可能就变得多，扩容也是一件非常耗费性能的一件事~

从源码上我们可以发现：HashMap并不是直接拿key的哈希值来用的，它会将key的哈希值的高16位进行异或操作，使得我们将元素放入哈希表的时候**增加了一定的随机性**。

还要值得注意的是：**并不是桶子上有8位元素的时候它就能变成红黑树，它得同时满足我们的散列表容量大于64才行的**~

**LinkedHashMap:**

LinkedHashMap比HashMap多了一个双向链表的维护，在数据结构而言它要复杂一些，阅读源码起来比较轻松一些，因为大多都由HashMap实现了..

阅读源码的时候我们会发现多态是无处不在的~子类用父类的方法，子类重写了父类的**部分**方法即可达到不一样的效果！

- 比如：LinkedHashMap并没有重写put方法，而put方法内部的newNode()方法重写了。LinkedHashMap调用父类的put方法，里面回调的是重写后的newNode()，从而达到目的！

LinkedHashMap可以设置两种遍历顺序：

- 访问顺序（access-ordered）
- 插入顺序（insertion-ordered）
- **默认是插入顺序的**

对于访问顺序，它是LRU(最近最少使用)算法的实现，要使用它要么**重写LinkedListMap的几个方法**(removeEldestEntry(Map.Entry eldest)和afterNodeInsertion(boolean evict))，要么是**扩展**成LRUMap来使用，不然设置为访问顺序（access-ordered）的用处不大~

**LinkedHashMap遍历的是内部维护的双向链表**，所以说初始容量对LinkedHashMap遍历是不受影响的

**TreeMap:**

- 由于底层是红黑树，那么时间复杂度可以保证为log(n)
- key不能为null，为null为抛出NullPointException的
- 想要自定义比较，在构造方法中传入Comparator对象，否则使用key的自然排序来进行比较
- TreeMap非同步的，想要同步可以使用Collections来进行封装

**Hashtable:**

- **每个方法上都加上了Synchronized**完成同步，效率低下，**初始容量11**

**ConcurrentHashMap**

- 在**部分加锁**和**利用CAS算法**来实现同步。
- **底层结构是散列表(数组+链表)+红黑树**，这一点和HashMap是一样的。

- Hashtable是将所有的方法进行同步，效率低下。而ConcurrentHashMap作为一个高并发的容器，它是通过**部分锁定+CAS算法来进行实现线程安全的**。CAS算法也可以认为是**乐观锁**的一种~
- 在高并发环境下，统计数据(计算size…等等)其实是无意义的，因为在下一时刻size值就变化了。
- get方法是非阻塞，无锁的。重写Node类，通过volatile修饰next来实现每次获取都是**最新**设置的值
- **ConcurrentHashMap的key和Value都不能为null**

**HashSet：**

- 无序，允许为null，底层是HashMap(散列表+红黑树)，非线程同步

**TreeSet：**

- 有序，不允许为null，底层是TreeMap(红黑树),非线程同步

**LinkedHashSet：**

- 迭代有序，允许为null，底层是HashMap+双向链表，非线程同步

**hashset如何实现存储的元素不重复？**

在向hashSet中add()元素时，判断元素是否存在的依据;

如果hash码值相同，且equles判断相等，说明元素已经存在